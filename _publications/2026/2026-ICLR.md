---
title:          "TokenSeek: Memory Efficient Fine Tuning via Instance-Aware Token Ditching"
date:           2026-01-26 12:01:00 +0800
selected:       true
pub:            "International Conference on Learning Representations"
# pub_pre:        "Submitted to "
# pub_post:       'Under review.'
pub_last:       ' <span class="badge badge-pill badge-publication badge-success">ICLR</span>'
pub_date:       "2026"
interests:      ["LLMs Training", "LLMs", "GenAI"]
note:           "Instruction Tuning"

abstract: >-
  Fine-tuning is a standard approach for adapting large language models to downstream tasks, but its efficiency is limited by high memory consumption, largely dominated by activations. We propose TokenSeek, a universal plugin for transformer-based models that performs instance-aware token seeking and ditching, achieving substantial memory savings (e.g., only 14.8% on Llama3.2 1B) while maintaining or even improving performance. Moreover, its interpretable token-seeking mechanism provides insights into why token efficiency can be improved.
cover:          assets/images/paper/ptallm.jpg
authors:
  - Runjia Zeng
  - Qifan Wang
  - Qiang Guan
  - Ruixiang Tang
  - Lifu Huang
  - Zhenting Wang
  - Xueling Zhang
  - Cheng Han
  - Dongfang Liu
links:
  HomepageðŸ’»: https://runjia.tech/iclr_tokenseek/
---
