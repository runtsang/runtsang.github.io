---
title:          "MEPT: Mixture of Experts Prompt Tuning as a Manifold Mapper"
date:           2025-08-20 12:01:00 +0800
selected:       true
pub:            "Conference on Empirical Methods in Natural Language Processing"
# pub_pre:        "Submitted to "
# pub_post:       'Under review.'
pub_last:       ' <span class="badge badge-pill badge-publication badge-success">EMNLP</span>'
pub_date:       "2025"

abstract: >-
  Considering deep neural networks as manifold mappers, the pretrain-then-fine-tune paradigm is a two-stage process: pretrain builds a broad knowledge base, and fine-tune adjusts parameters to activate specific neural pathways aligning with the target manifold. The rigid parameter space constrain of prior prompt tuning methods limits dynamic pathway activation, making them less adaptable to diverse and evolving data. In this view, we propose Mixture of Expert Prompt Tuning (MEPT) that leverages multiple prompt experts to adaptively learn diverse and non-stationary data distributions.
cover:          assets/images/paper/mept.jpg
authors:
  - Runjia Zeng
  - Guangyan Sun
  - Qifan Wang
  - Tong Geng
  - Sohail Dianat
  - Xiaotian Han
  - Raghuveer Rao
  - Xueling Zhang
  - Cheng Han
  - Lifu Huang
  - Dongfang Liu
links:
  HomepageðŸ’»: https://runjia.tech/emnlp_mept/
  CodeðŸ’¾: https://github.com/runtsang/MEPT
  PaperðŸ“‘: https://arxiv.org/abs/2509.00996
---
