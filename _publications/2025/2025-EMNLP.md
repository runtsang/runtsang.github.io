---
title:          "MEPT: Mixture of Experts Prompt Tuning as a Manifold Mapper"
date:           2025-08-20 12:01:00 +0800
selected:       true
pub:            "Conference on Empirical Methods in Natural Language Processing"
# pub_pre:        "Submitted to "
# pub_post:       'Under review.'
pub_last:       ' <span class="badge badge-pill badge-publication badge-success">EMNLP</span>'
pub_date:       "2025"

abstract: >-
  Considering deep neural networks as manifold mappers, the pretrain-then-fine-tune paradigm can be interpreted as a two-stage process: pretrain establishes a broad knowledge base, and fine-tune adjusts the model parameters to activate specific neural pathways to align with the target manifold. Although prior fine-tuning approaches demonstrate success, their rigid parameter space limits their ability to dynamically activate appropriate neural pathways, rendering them ill-equipped to adapt flexibly to the diverse and evolving data distributions. In light of this view, we propose a novel approach, Mixture of Expert Prompt Tuning (MEPT), as an effective and efficient manifold-mapping framework. MEPT leverages the Mixture of Experts architecture by integrating multiple prompt experts to adaptively learn diverse and non-stationary data distributions.
cover:          assets/images/paper/mept.jpg
authors:
  - Runjia Zeng
  - Guangyan Sun
  - Qifan Wang
  - Tong Geng
  - Sohail Dianat
  - Xiaotian Han
  - Raghuveer Rao
  - Xueling Zhang
  - Cheng Han
  - Lifu Huang
  - Dongfang Liu
links:
  Homepage: https://runjia.tech/mept_page/
---
